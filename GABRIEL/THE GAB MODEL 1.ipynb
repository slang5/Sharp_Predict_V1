{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f848896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "443737e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "324df8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_init = pd.read_csv('data/initial_data.csv', header=[0, 1], index_col=0, parse_dates=True).copy()\n",
    "data_base_init.head()\n",
    "\n",
    "# Sort by date index\n",
    "data_base_init = data_base_init.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdbf0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding features : rolling mean and rolling std + binary features based on rolling mean\n",
    "\n",
    "rolling_windows = [5, 10, 20, 50, 75, 100]\n",
    "feature_list = ['Price', 'Volume', 'High_over_Low', 'Day_yield']\n",
    "symbol_list = data_base_init.columns.get_level_values(0).unique()\n",
    "\n",
    "new_columns = {}\n",
    "\n",
    "for window in rolling_windows:\n",
    "    for feature in feature_list:\n",
    "        for symbol in symbol_list:\n",
    "            col_name = (symbol, f'{feature}_rolling_mean_{window}')\n",
    "            new_columns[col_name] = data_base_init[symbol][feature].rolling(window=window).mean()\n",
    "            \n",
    "            col_name_std = (symbol, f'{feature}_rolling_std_{window}')\n",
    "            new_columns[col_name_std] = data_base_init[symbol][feature].rolling(window=window).std()\n",
    "            \n",
    "            # Binary feature: 1 if Price > rolling mean, else 0\n",
    "            col_name_bin = (symbol, f'{feature}_above_rolling_mean_{window}')\n",
    "            new_columns[col_name_bin] = (data_base_init[symbol][feature] > new_columns[col_name]).astype(int)\n",
    "\n",
    "# Concatenate all new columns at once to avoid fragmentation\n",
    "data_features = pd.concat([data_base_init, pd.DataFrame(new_columns)], axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a71b8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 100 first days due to rolling features and avoiding NaN values\n",
    "n = 100 + 1\n",
    "first_day = data_features.index[n]\n",
    "data_features = data_features[data_features.index>first_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc29f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill forward on any remaining NaN values or 0 if needed\n",
    "data_features = data_features.ffill()\n",
    "data_features = data_features.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd1c3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features per symbol: 76\n"
     ]
    }
   ],
   "source": [
    "# n = number of features for each symbol\n",
    "n = data_features.shape[1] // len(symbol_list)\n",
    "print(f'Total features per symbol: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc7791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep most interestsing features \n",
    "\n",
    "list_features = []\n",
    "\n",
    "for symbol in symbol_list:\n",
    "    feature_cols = [col for col in data_features[symbol].columns if col != 'Day_yield']\n",
    "    X_symbol = data_features[symbol][feature_cols]\n",
    "    y_symbol = (data_features[symbol]['Day_yield']>0).astype(int)\n",
    "\n",
    "    # Variance Threshold to remove low variance features\n",
    "    var_thresh = VarianceThreshold(threshold=0.00)\n",
    "    X_var = var_thresh.fit_transform(X_symbol)\n",
    "\n",
    "    # SelectKBest to select top 40 features\n",
    "    selector = SelectKBest(score_func=f_classif, k=n // 6)\n",
    "    X_new = selector.fit_transform(X_var, y_symbol)\n",
    "\n",
    "    selected_features = X_symbol.columns[var_thresh.get_support()][selector.get_support()]\n",
    "   \n",
    "    list_features.extend(selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eede746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting feature occurrences + Select features that were selected more than once\n",
    "feature_counts = Counter(list_features)\n",
    "\n",
    "final_features = [feature for feature, count in feature_counts.items() if count > 1]\n",
    "final_features.append('Day_yield')\n",
    "final_features = sorted(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c611968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only selected features in data_selected_features\n",
    "data_selected_features = data_features[[(symbol, feature) for symbol in symbol_list for feature in final_features]]\n",
    "data_selected_features.columns.names = ['Symbol', 'Feature']\n",
    "\n",
    "data_selected_features.to_csv('data/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e08e4a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "Day_yield                               4931\n",
       "Day_yield_above_rolling_mean_10         4931\n",
       "Day_yield_above_rolling_mean_100        4931\n",
       "Day_yield_above_rolling_mean_20         4931\n",
       "Day_yield_above_rolling_mean_5          4931\n",
       "Day_yield_above_rolling_mean_50         4931\n",
       "Day_yield_above_rolling_mean_75         4931\n",
       "Day_yield_rolling_mean_10               4931\n",
       "Day_yield_rolling_mean_20               4931\n",
       "Day_yield_rolling_mean_5                4931\n",
       "High_over_Low                           4931\n",
       "High_over_Low_above_rolling_mean_10     4931\n",
       "High_over_Low_above_rolling_mean_100    4931\n",
       "High_over_Low_above_rolling_mean_20     4931\n",
       "High_over_Low_above_rolling_mean_5      4931\n",
       "High_over_Low_above_rolling_mean_50     4931\n",
       "High_over_Low_above_rolling_mean_75     4931\n",
       "Price                                   4931\n",
       "Price_above_rolling_mean_10             4931\n",
       "Price_above_rolling_mean_20             4931\n",
       "Price_above_rolling_mean_5              4931\n",
       "Price_above_rolling_mean_50             4931\n",
       "Price_rolling_mean_50                   4931\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_selected_features.ALJJ.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33e70147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let try a light neural network on the few selected features\n",
    "\n",
    "few_features = ['Day_yield', 'Day_yield_above_rolling_mean_10', 'High_over_Low_above_rolling_mean_10', 'Price_rolling_mean_50']\n",
    "\n",
    "light_data = data_selected_features[[(symbol, feature) for symbol in symbol_list for feature in few_features]]\n",
    "light_data.columns.names = ['Symbol', 'Feature']\n",
    "\n",
    "# Save light data\n",
    "light_data.to_csv('data/light_processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "add6e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets and X and y for each symbol\n",
    "\n",
    "# X is : for each stock => 'Day_yield_above_rolling_mean_10', 'High_over_Low_above_rolling_mean_10', 'Price_rolling_mean_50'\n",
    "# Y is : 'Day_yield' shifted by -1 day (next day) and > 0 as binary classification in int format\n",
    "\n",
    "X = pd.DataFrame()\n",
    "Y = pd.DataFrame()\n",
    "\n",
    "for symbol in symbol_list:\n",
    "    X_symbol = light_data[symbol][['Day_yield_above_rolling_mean_10', 'High_over_Low_above_rolling_mean_10', 'Price_rolling_mean_50']]\n",
    "    y_symbol = (light_data[symbol]['Day_yield'].shift(-1) > 0).astype(int)\n",
    "    \n",
    "    X_symbol.columns = pd.MultiIndex.from_tuples([(symbol, col) for col in X_symbol.columns])\n",
    "    X = pd.concat([X, X_symbol], axis=1)\n",
    "    Y = pd.concat([Y, y_symbol.rename(symbol)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12acefa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat X with symbols as prefix in column names\n",
    "X_flat = pd.DataFrame()\n",
    "for symbol in symbol_list:\n",
    "    X_symbol = X[symbol]\n",
    "    X_symbol.columns = [f'{symbol}_{col}' for col in X_symbol.columns]\n",
    "    X_flat = pd.concat([X_flat, X_symbol], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b4c3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(X_flat.index)\n",
    "end_train_date = dates[int(len(dates)*0.6)]\n",
    "end_test_date = dates[int(len(dates)*0.8)] \n",
    "\n",
    "\n",
    "\n",
    "X_train = X_flat[X_flat.index <= end_train_date]\n",
    "Y_train = Y[Y.index <= end_train_date]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "X_test = X_flat[X_flat.index > end_train_date]\n",
    "Y_test = Y[Y.index > end_train_date]\n",
    "X_test = X_test[X_test.index <= end_test_date]\n",
    "Y_test = Y_test[Y_test.index <= end_test_date]\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "X_validation = X_flat[X_flat.index > end_test_date]\n",
    "Y_validation = Y[Y.index > end_test_date]\n",
    "\n",
    "X_validation = pd.DataFrame(scaler.transform(X_validation), index=X_validation.index, columns=X_validation.columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "29a4d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.0223 - loss: 0.6785 - val_accuracy: 0.0000e+00 - val_loss: 0.6798\n",
      "Epoch 2/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.6693 - val_accuracy: 0.0000e+00 - val_loss: 0.6788\n",
      "Epoch 3/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6687 - val_accuracy: 0.0000e+00 - val_loss: 0.6802\n",
      "Epoch 4/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6679 - val_accuracy: 0.0000e+00 - val_loss: 0.6789\n",
      "Epoch 5/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.6670 - val_accuracy: 0.0000e+00 - val_loss: 0.6844\n",
      "Epoch 6/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0027 - loss: 0.6659 - val_accuracy: 0.0101 - val_loss: 0.6830\n",
      "Epoch 7/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0108 - loss: 0.6647 - val_accuracy: 0.0051 - val_loss: 0.6903\n",
      "Epoch 8/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0081 - loss: 0.6631 - val_accuracy: 0.0061 - val_loss: 0.6947\n",
      "Epoch 9/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0071 - loss: 0.6618 - val_accuracy: 0.0091 - val_loss: 0.6869\n",
      "Epoch 10/10\n",
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.0047 - loss: 0.6603 - val_accuracy: 0.0020 - val_loss: 0.6963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17b07cce0d0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a simple neural network model with TensorFlow\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='sigmoid'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='sigmoid'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='sigmoid'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(Y_train.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=16, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    }
   ],
   "source": [
    "result = model.predict(X_validation).round()\n",
    "save = {}\n",
    "for i in range(1, len(Y_validation)):\n",
    "    save[Y_validation.index[i]] = accuracy_score(Y_validation.iloc[i].tolist(), result[i])\n",
    "\n",
    "mean = np.mean(list(save.values()))\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "422ac59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of neural network model on full data with all selected features\n",
    "\n",
    "X_full = pd.DataFrame()\n",
    "Y_full = pd.DataFrame()\n",
    "for symbol in symbol_list:\n",
    "    X_symbol = data_selected_features[symbol].drop(columns=['Day_yield'])\n",
    "    y_symbol = (data_selected_features[symbol]['Day_yield'].shift(-1) > 0).astype(int)\n",
    "    \n",
    "    X_symbol.columns = pd.MultiIndex.from_tuples([(symbol, col) for col in X_symbol.columns])\n",
    "    X_full = pd.concat([X_full, X_symbol], axis=1)\n",
    "    Y_full = pd.concat([Y_full, y_symbol.rename(symbol)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0c906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat X with symbols as prefix in column names\n",
    "X_full_flat = pd.DataFrame()\n",
    "for symbol in symbol_list:\n",
    "    X_symbol = X_full[symbol]\n",
    "    X_symbol.columns = [f'{symbol}_{col}' for col in X_symbol.columns]\n",
    "    X_full_flat = pd.concat([X_full_flat, X_symbol], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb58d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sorted(X_full_flat.index)\n",
    "end_train_date = dates[int(len(dates)*0.6)]\n",
    "end_test_date = dates[int(len(dates)*0.8)] \n",
    "\n",
    "X_train = X_full_flat[X_full_flat.index <= end_train_date]\n",
    "Y_train = Y_full[Y_full.index <= end_train_date]\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "X_test = X_full_flat[X_full_flat.index > end_train_date]\n",
    "Y_test = Y_full[Y_full.index > end_train_date]\n",
    "X_test = X_test[X_test.index <= end_test_date]\n",
    "Y_test = Y_test[Y_test.index <= end_test_date]\n",
    "\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "X_validation = X_full_flat[X_full_flat.index > end_test_date]\n",
    "Y_validation = Y_full[Y_full.index > end_test_date]\n",
    "\n",
    "X_validation = pd.DataFrame(scaler.transform(X_validation), index=X_validation.index, columns=X_validation.columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f25a2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6715 - val_accuracy: 0.0000e+00 - val_loss: 0.6819\n",
      "Epoch 2/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6701 - val_accuracy: 0.0000e+00 - val_loss: 0.6793\n",
      "Epoch 3/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6698 - val_accuracy: 0.0000e+00 - val_loss: 0.6776\n",
      "Epoch 4/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.6697 - val_accuracy: 0.0000e+00 - val_loss: 0.6788\n",
      "Epoch 5/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.6695 - val_accuracy: 0.0000e+00 - val_loss: 0.6806\n",
      "Epoch 6/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6696 - val_accuracy: 0.0000e+00 - val_loss: 0.6795\n",
      "Epoch 7/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6695 - val_accuracy: 0.0000e+00 - val_loss: 0.6787\n",
      "Epoch 8/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6695 - val_accuracy: 0.0000e+00 - val_loss: 0.6787\n",
      "Epoch 9/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6694 - val_accuracy: 0.0000e+00 - val_loss: 0.6787\n",
      "Epoch 10/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.6694 - val_accuracy: 0.0000e+00 - val_loss: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x196207cc380>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a neural network model with TensorFlow\n",
    "\n",
    "model_full = models.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='sigmoid'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='sigmoid'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(16, activation='sigmoid'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(32, activation='sigmoid'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(64, activation='sigmoid'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(Y_train.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_full.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_full.fit(X_train, Y_train, epochs=10, batch_size=2, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16f3fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.542766497461929)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model_full.predict(X_validation).round()\n",
    "save = {}\n",
    "for i in range(1, len(Y_validation)):\n",
    "    save[Y_validation.index[i]] = accuracy_score(Y_validation.iloc[i].tolist(), result[i])\n",
    "\n",
    "mean = np.mean(list(save.values()))\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90237226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.0014 - loss: 0.6696 - val_accuracy: 0.0000e+00 - val_loss: 0.6761\n",
      "Epoch 2/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 25ms/step - accuracy: 0.0068 - loss: 0.6535 - val_accuracy: 0.0233 - val_loss: 0.6795\n",
      "Epoch 3/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 26ms/step - accuracy: 0.0068 - loss: 0.6348 - val_accuracy: 0.0122 - val_loss: 0.6902\n",
      "Epoch 4/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 27ms/step - accuracy: 0.0145 - loss: 0.6105 - val_accuracy: 0.0162 - val_loss: 0.7059\n",
      "Epoch 5/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 27ms/step - accuracy: 0.0152 - loss: 0.5876 - val_accuracy: 0.0193 - val_loss: 0.7183\n",
      "Epoch 6/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 27ms/step - accuracy: 0.0179 - loss: 0.5682 - val_accuracy: 0.0071 - val_loss: 0.7383\n",
      "Epoch 7/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 28ms/step - accuracy: 0.0162 - loss: 0.5507 - val_accuracy: 0.0162 - val_loss: 0.7513\n",
      "Epoch 8/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 29ms/step - accuracy: 0.0176 - loss: 0.5368 - val_accuracy: 0.0112 - val_loss: 0.7643\n",
      "Epoch 9/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 28ms/step - accuracy: 0.0196 - loss: 0.5232 - val_accuracy: 0.0132 - val_loss: 0.7762\n",
      "Epoch 10/10\n",
      "\u001b[1m1480/1480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 27ms/step - accuracy: 0.0176 - loss: 0.5121 - val_accuracy: 0.0112 - val_loss: 0.7937\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.512357233502538)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a neural network model with TensorFlow\n",
    "\n",
    "model_full = models.Sequential([\n",
    "    layers.Input(shape=(1, X_train.shape[1])),\n",
    "    \n",
    "    layers.LSTM(256, return_sequences=True),\n",
    "    #layers.LSTM(64, return_sequences=True),\n",
    "    layers.LSTM(32),\n",
    "    #layers.Dropout(0.05),\n",
    "    #layers.Dense(64, activation='relu'),\n",
    "    #layers.Dense(32, activation='relu'),\n",
    "    #layers.Dense(16, activation='relu'),\n",
    "    layers.Dropout(0.05),\n",
    "    layers.Dense(Y_train.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_full.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape data for LSTM (add timestep dimension)\n",
    "X_train_reshaped = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_validation_reshaped = X_validation.values.reshape((X_validation.shape[0], 1, X_validation.shape[1]))\n",
    "\n",
    "model_full.fit(X_train_reshaped, Y_train, epochs=10, batch_size=2, validation_data=(X_test_reshaped, Y_test))\n",
    "\n",
    "result = model_full.predict(X_validation_reshaped).round()\n",
    "save = {}\n",
    "for i in range(1, len(Y_validation)):\n",
    "    save[Y_validation.index[i]] = accuracy_score(Y_validation.iloc[i].tolist(), result[i])\n",
    "\n",
    "mean = np.mean(list(save.values()))\n",
    "mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
